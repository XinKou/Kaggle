import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn import metrics

from multiprocessing import *
import gc
import warnings
warnings.filterwarnings("ignore")

import xgboost as xgb
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression


#### Load Data
train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')

### Preprocessing
y = train['target'].values
testid= test['id'].values

train.drop(['id','target'],axis=1,inplace=True)
test.drop(['id'],axis=1,inplace=True)


######################################## Feature engineering ##########################################

### Drop calc
unwanted = train.columns[train.columns.str.startswith('ps_calc_')]
train = train.drop(unwanted, axis=1)  
test = test.drop(unwanted, axis=1)

###

#train = train.replace(-1, np.nan)
#test = test.replace(-1, np.nan)

### Reconstruct the feature 'ps_reg_03', discovered by Pascal https://www.kaggle.com/pnagel/reconstruction-of-ps-reg-03
def recon(reg):
    integer = int(np.round((40*reg)**2)) 
    for a in range(32):
        if (integer - a) % 31 == 0:
            A = a
    M = (integer - A)//31
    return A, M
train['ps_reg_A'] = train['ps_reg_03'].apply(lambda x: recon(x)[0])
train['ps_reg_M'] = train['ps_reg_03'].apply(lambda x: recon(x)[1])
train['ps_reg_A'].replace(19,-1, inplace=True)
train['ps_reg_M'].replace(51,-1, inplace=True)
test['ps_reg_A'] = test['ps_reg_03'].apply(lambda x: recon(x)[0])
test['ps_reg_M'] = test['ps_reg_03'].apply(lambda x: recon(x)[1])
test['ps_reg_A'].replace(19,-1, inplace=True)
test['ps_reg_M'].replace(51,-1, inplace=True)


### Froza's baseline, https://www.kaggle.com/the1owl/forza-baseline

d_median = train.median(axis=0)
d_mean = train.mean(axis=0)
d_skew = train.skew(axis=0)
one_hot = {c: list(train[c].unique()) for c in train.columns if c not in ['id','target']}

def transform_df(df):
    df = pd.DataFrame(df)
    dcol = [c for c in df.columns if c not in ['id','target']]
    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']
    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)
    for c in dcol:
        if '_bin' not in c: #standard arithmetic
            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(np.int)
            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(np.int)

    for c in one_hot:
        if len(one_hot[c])>2 and len(one_hot[c]) < 7:
            for val in one_hot[c]:
                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)
    return df

def multi_transform(df):
    print('Init Shape: ', df.shape)
    p = Pool(cpu_count())
    df = p.map(transform_df, np.array_split(df, cpu_count()))
    df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)
    p.close(); p.join()
    print('After Shape: ', df.shape)
    return df

train = multi_transform(train)
test = multi_transform(test)



###################################### Metric################################################
### Fast Gini calculation, https://www.kaggle.com/tezdhar/faster-gini-calculation

def ginic(actual, pred):
    actual = np.asarray(actual) #In case, someone passes Series or list
    n = len(actual)
    a_s = actual[np.argsort(pred)]
    a_c = a_s.cumsum()
    giniSum = a_c.sum() / a_s.sum() - (n + 1) / 2.0
    return giniSum / n
 
def gini_normalizedc(a, p):
    if p.ndim == 2:#Required for sklearn wrapper
        p = p[:,1] #If proba array contains proba for both 0 and 1 classes, just pick class 1
    return ginic(a, p) / ginic(a, a)

def gini_xgb(preds, dtrain):
    labels = dtrain.get_label()
    gini_score = gini_normalizedc(labels, preds)
    return 'gini', gini_score

gini_sklearnf = metrics.make_scorer(gini_normalizedc, True, True) # for sklearn wrapper


# class Ensemble(object):
#     def __init__(self, n_splits, stacker, base_models):
#         self.n_splits = n_splits
#         self.stacker = stacker
#         self.base_models = base_models

#     def fit_predict(self, X, y, T):
#         X = np.array(X)
#         y = np.array(y)
#         T = np.array(T)

#         folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))

#         S_train = np.zeros((X.shape[0], len(self.base_models)))
#         S_test = np.zeros((T.shape[0], len(self.base_models)))
#         for i, clf in enumerate(self.base_models):

#             S_test_i = np.zeros((T.shape[0], self.n_splits))

#             for j, (train_idx, test_idx) in enumerate(folds):
#                 X_train = X[train_idx]
#                 y_train = y[train_idx]
#                 X_holdout = X[test_idx]
# #                y_holdout = y[test_idx]

#                 print ("Fit %s fold %d" % (str(clf).split('(')[0], j+1))
#                 clf.fit(X_train, y_train)
# #                cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')
# #                print("    cross_score: %.5f" % (cross_score.mean()))
#                 y_pred = clf.predict_proba(X_holdout)[:,1]                

#                 S_train[test_idx, i] = y_pred
#                 S_test_i[:, j] = clf.predict_proba(T)[:,1]
#             S_test[:, i] = S_test_i.mean(axis=1)

#         results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')
#         print("Stacker score: %.5f" % (results.mean()))

#         self.stacker.fit(S_train, y)
#         res = self.stacker.predict_proba(S_test)[:,1]
#         return res




# # LightGBM params
# lgb_params = {}
# lgb_params['learning_rate'] = 0.02
# lgb_params['n_estimators'] = 650
# lgb_params['max_bin'] = 10
# lgb_params['subsample'] = 0.8
# lgb_params['subsample_freq'] = 10
# lgb_params['colsample_bytree'] = 0.8   
# lgb_params['min_child_samples'] = 500
# lgb_params['seed'] = 99


# lgb_params2 = {}
# lgb_params2['n_estimators'] = 1090
# lgb_params2['learning_rate'] = 0.02
# lgb_params2['colsample_bytree'] = 0.3   
# lgb_params2['subsample'] = 0.7
# lgb_params2['subsample_freq'] = 2
# lgb_params2['num_leaves'] = 16
# lgb_params2['seed'] = 99


# lgb_params3 = {}
# lgb_params3['n_estimators'] = 1100
# lgb_params3['max_depth'] = 4
# lgb_params3['learning_rate'] = 0.02
# lgb_params3['seed'] = 99


# lgb_model = LGBMClassifier(**lgb_params)

# lgb_model2 = LGBMClassifier(**lgb_params2)

# lgb_model3 = LGBMClassifier(**lgb_params3)



# log_model = LogisticRegression()


        
# stack = Ensemble(n_splits=3,
#         stacker = log_model,
#         base_models = (lgb_model, lgb_model2, lgb_model3))        
        
# y_pred = stack.fit_predict(train, y, test) 

# sub = pd.DataFrame()
# sub['id'] = testid
# sub['target'] = y_pred
# sub.to_csv('stacked_lgb.csv', index=False)
